import os, time, json, io
import streamlit as st
from PIL import Image
from typing import Dict, Any, List

from src.utils import (
    get_logger, MODEL_ID, TEMPERATURE, MAX_TOKENS, MOCK_MODE,
    LIVE_IMAGE_ALWAYS, to_base64
)
# üëá th√™m c√°c builder m·ªõi cho Nova & Jamba
from src.prompt_builder import (
    build_prompt, build_prompt_with_image,
    build_prompt_nova, build_prompt_jamba
)
from src.parser import parse_and_validate, extract_text
from src.schema import Dish

logger = get_logger("ui")

st.set_page_config(page_title="Demo LLM ‚Üí JSON m√≥n ƒÉn", page_icon="üçú", layout="wide")
st.title("üçú Demo: M√¥ t·∫£ m√≥n ƒÉn ‚Üí JSON nguy√™n li·ªáu")

# Enable auto-rerun for development
if 'debug_mode' not in st.session_state:
    st.session_state.debug_mode = True

# ====== SIDEBAR ======
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")
st.sidebar.info("Mock mode hi·ªán ƒëang **B·∫¨T** ƒë·ªÉ ph√π h·ª£p demo th·ª±c t·∫ø m√† kh√¥ng c·∫ßn AWS.")
st.sidebar.write(f"MOCK_MODE = {'‚úÖ' if MOCK_MODE else '‚ùå'}")

with st.sidebar.expander("üêõ Debug Mode"):
    from src.utils import REGION
    auto_rerun = st.checkbox("Auto-rerun on file change", value=True)
    show_debug_info = st.checkbox("Show debug info", value=False)
    if show_debug_info:
        st.write("**Config:**")
        st.write(f"- MODEL_ID: `{MODEL_ID}`")
        st.write(f"- REGION: `{REGION}`")
        st.write(f"- MOCK_MODE: `{MOCK_MODE}`")
        st.write(f"- LIVE_IMAGE_ALWAYS: `{LIVE_IMAGE_ALWAYS}`")
        st.write("Session state:", st.session_state)

# ========= Client (LIVE) =========
bedrock_client = None
# Kh·ªüi t·∫°o client khi c·∫ßn LIVE (text khi MOCK=false ho·∫∑c ·∫£nh khi LIVE_IMAGE_ALWAYS=true)
if (not MOCK_MODE) or LIVE_IMAGE_ALWAYS:
    try:
        from src.bedrock_client import BedrockClient
        bedrock_client = BedrockClient()
    except Exception as e:
        st.sidebar.error(f"Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c Bedrock client (LIVE): {e}")

# ========= Helpers (MOCK) =========
def mock_response_for_text(desc: str) -> Dict[str, Any]:
    # GI·ªÆ NGUY√äN INGREDIENTS B·∫†N ƒê√É S·ª¨A
    mock_json = {
        "dish_name": "Ph·ªü b√≤" if "ph·ªü" in desc.lower() else "M√≥n ƒÉn",
        "cuisine": "Vietnamese",
        "ingredients": [
            {"name": "b√°nh ph·ªü", "quantity": "200 g", "unit": "g"},
            {"name": "th·ªãt b√≤ thƒÉn", "quantity": "250 g", "unit": "g"},
            {"name": "n∆∞·ªõc d√πng b√≤", "quantity": "500 ml", "unit": "ml"},
            {"name": "h√†nh l√°", "quantity": "2 nh√°nh", "unit": None},
            {"name": "qu·∫ø", "quantity": "1 thanh", "unit": None},
            {"name": "g·ª´ng", "quantity": "1 c·ªß nh·ªè", "unit": None},
            {"name": "rau m√πi", "quantity": "1 √≠t", "unit": None}
        ],
        "notes": ["Kh·∫©u ph·∫ßn v√† ƒë·ªãnh l∆∞·ª£ng c√≥ th·ªÉ thay ƒë·ªïi theo nhu c·∫ßu"]
    }
    return {"content": [{"type": "text", "text": json.dumps(mock_json, ensure_ascii=False)}]}

def mock_response_for_image(desc: str, img: Image.Image) -> Dict[str, Any]:
    # Heuristic nh·ªè ƒë·ªÉ demo: n·∫øu ·∫£nh l·ªõn -> th√™m 'h√†nh t√¢y'
    w, h = img.size
    extra = {"name": "h√†nh t√¢y", "quantity": "1/2 c·ªß", "unit": None} if max(w, h) > 512 else None
    base = mock_response_for_text(desc)
    data = json.loads(extract_text(base))
    if extra:
        data["ingredients"].append(extra)
    return {"content": [{"type": "text", "text": json.dumps(data, ensure_ascii=False)}]}

# ========= Body router & Normalizer =========
def build_body_for_model(model_id: str, desc: str, temperature: float, max_tokens: int) -> dict:
    # Debug logging to see what's being passed
    logger.info(f"Building prompt for model: {model_id}")
    
    mid = (model_id or "").strip().lower()

    # Claude models: check for 'anthropic' or 'claude'
    if 'anthropic' in mid or 'claude' in mid:
        logger.info(f"Using Claude prompt builder for {model_id}")
        return build_prompt(desc, temperature=temperature, max_tokens=max_tokens)
    
    # Nova models: amazon.nova-* 
    if 'nova' in mid:
        logger.info(f"Using Nova prompt builder for {model_id}")
        return build_prompt_nova(desc, temperature=temperature, max_tokens=max_tokens)

    # Jamba 1.5 models: ai21.jamba*
    if 'jamba' in mid or 'ai21' in mid:
        logger.info(f"Using Jamba prompt builder for {model_id}")
        return build_prompt_jamba(desc, temperature=temperature, max_tokens=max_tokens)

    # Fallback to Claude format (not ideal but for safety)
    logger.warning(f"Unknown model type: {model_id}, falling back to Claude format")
    return build_prompt(desc, temperature=temperature, max_tokens=max_tokens)


def normalize_to_claude_like(raw: Dict[str, Any]) -> Dict[str, Any]:
    """
    B·ªçc response v·ªÅ d·∫°ng {"content":[{"type":"text","text":"..."}]} ƒë·ªÉ t√°i d√πng parse_and_validate().
    - Claude: ƒë√£ ƒë√∫ng -> tr·∫£ nguy√™n
    - Nova/Jamba: c·ªë g·∫Øng l·∫•y outputText ho·∫∑c c√°c field ph·ªï bi·∫øn -> b·ªçc l·∫°i
    """
    if isinstance(raw, dict) and "content" in raw:
        return raw

    txt = None
    if isinstance(raw, dict):
        # Nova/Jamba th∆∞·ªùng c√≥ 'outputText'
        txt = raw.get("outputText")
        if not txt:
            # d√≤ v√†i key ph·ªï bi·∫øn kh√°c
            for key in ("result", "output", "completion", "text"):
                val = raw.get(key)
                if isinstance(val, str) and val.strip():
                    txt = val.strip()
                    break
    if not txt:
        # fallback: stringify to√†n b·ªô raw
        txt = json.dumps(raw, ensure_ascii=False)

    return {"content": [{"type": "text", "text": txt}]}

# ========= Invoke =========
def invoke_text(desc: str, temperature: float, max_tokens: int) -> Dict[str, Any]:
    if MOCK_MODE:
        return mock_response_for_text(desc)
    if not bedrock_client or not MODEL_ID:
        raise RuntimeError("Bedrock client/MODEL_ID ch∆∞a s·∫µn s√†ng")
    body = build_body_for_model(MODEL_ID, desc, temperature, max_tokens)
    raw = bedrock_client.invoke(model_id=MODEL_ID, body=body)
    return normalize_to_claude_like(raw)

def invoke_image(desc: str, img: Image.Image, temperature: float, max_tokens: int) -> Dict[str, Any]:
    """
    ·∫¢nh ∆∞u ti√™n LIVE ngay c·∫£ khi text ƒëang MOCK; thi·∫øu credentials -> fallback MOCK ƒë·ªÉ kh√¥ng v·ª° demo.
    (·∫¢nh hi·ªán d√πng builder Claude multimodal; n·∫øu sau b·∫°n mu·ªën d√πng Nova/Jamba-vision th√¨ ƒë·ªïi router t∆∞∆°ng ·ª©ng)
    """
    if bedrock_client and LIVE_IMAGE_ALWAYS and MODEL_ID:
        try:
            buf = io.BytesIO()
            img_format = "PNG"
            mime = "image/png"
            img.save(buf, img_format)
            b64 = to_base64(buf.getvalue())
            body = build_prompt_with_image(desc, b64, mime, temperature=temperature, max_tokens=max_tokens)
            raw = bedrock_client.invoke(model_id=MODEL_ID, body=body)
            return normalize_to_claude_like(raw)
        except Exception as e:
            st.warning(f"·∫¢nh LIVE l·ªói, fallback MOCK: {e}")
            return mock_response_for_image(desc, img)
    # fallback MOCK
    return mock_response_for_image(desc, img)

# ========= Render =========
def render_result(raw_response: Dict[str, Any], t0: float, mode_label: str):
    try:
        dish: Dish = parse_and_validate(raw_response)
        elapsed = time.time() - t0
        st.success("‚úÖ JSON h·ª£p l·ªá theo schema")
        st.json(dish.model_dump(mode="json"))
        st.caption(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {elapsed:.2f}s ‚Ä¢ Ch·∫ø ƒë·ªô: {mode_label}")
        with st.expander("üîé Raw response"):
            st.code(json.dumps(raw_response, ensure_ascii=False, indent=2), language="json")
    except Exception as e:
        st.error(f"‚ùå Parse/Validate th·∫•t b·∫°i: {e}")

# ========= Benchmark =========
def benchmark_models(desc: str, models: List[str], temperature: float, max_tokens: int) -> List[Dict[str, Any]]:
    results = []
    for mid in models:
        t0 = time.time()
        try:
            if MOCK_MODE:
                base = mock_response_for_text(desc)
                latency = 0.5 + 0.3 * (hash(mid) % 5) / 5
                time.sleep(min(latency, 1.2))
                text = extract_text(base)
                valid = True
                tokens_in = 120 + (hash(mid) % 50)
                tokens_out = len(text) // 4
                cost_est = round(tokens_in/1e6*0.8 + tokens_out/1e6*1.2, 6)
                results.append({
                    "model": mid,
                    "latency_s": round(time.time()-t0, 2),
                    "valid_rate": 1.0 if valid else 0.0,
                    "tokens_in": tokens_in,
                    "tokens_out": tokens_out,
                    "cost_est_usd": cost_est,
                })
            else:
                if not bedrock_client:
                    raise RuntimeError("Bedrock client ch∆∞a s·∫µn s√†ng")
                # üîÅ ch·ªçn builder ƒë√∫ng theo t·ª´ng model
                body = build_body_for_model(mid, desc, temperature, max_tokens)
                raw = bedrock_client.invoke(model_id=mid, body=body)
                raw_norm = normalize_to_claude_like(raw)
                _ = parse_and_validate(raw_norm)
                results.append({
                    "model": mid,
                    "latency_s": round(time.time()-t0, 2),
                    "valid_rate": 1.0,
                    "tokens_in": 0,
                    "tokens_out": 0,
                    "cost_est_usd": 0.0,
                })
        except Exception as e:
            results.append({
                "model": mid,
                "latency_s": round(time.time()-t0, 2),
                "valid_rate": 0.0,
                "tokens_in": 0,
                "tokens_out": 0,
                "cost_est_usd": 0.0,
                "error": str(e)
            })
    return results

# ========= UI TABS =========
tab_extract, tab_bench = st.tabs(["üç≤ Extract JSON", "üèÅ Benchmark"])

with tab_extract:
    st.subheader("Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠p: ch·ªâ **m·ªôt** trong hai")
    input_mode = st.radio("Input mode", ["Text", "Image"], horizontal=True)

    with st.expander("üìã H∆∞·ªõng d·∫´n", expanded=True):
        st.markdown(
            "- **Text**: Nh·∫≠p m√¥ t·∫£ m√≥n ƒÉn ‚Üí m√¥ h√¨nh tr√≠ch JSON.\n"
            "- **Image**: T·∫£i ·∫£nh m√≥n ƒÉn ‚Üí m√¥ h√¨nh tr√≠ch JSON **t·ª´ ·∫£nh** (kh√¥ng d√πng m√¥ t·∫£ text).\n"
            "- Ch·ªâ d√πng **m·ªôt** ngu·ªìn ƒë·∫ßu v√†o t·∫°i m·ªôt th·ªùi ƒëi·ªÉm."
        )

    # ƒêi·ªÅu khi·ªÉn chung
    c1, c2, c3 = st.columns(3)
    with c1:
        temperature = st.number_input("Temperature", 0.0, 1.0, TEMPERATURE, 0.1)
    with c2:
        max_tokens = st.number_input("Max tokens", 64, 4096, MAX_TOKENS, 64)
    with c3:
        run_extract = st.button("Extract Ingredients", type="primary")

    # ===== Mode TEXT =====
    if input_mode == "Text":
        user_desc = st.text_area("M√¥ t·∫£ m√≥n ƒÉn", value="H√£y cho t√¥i nguy√™n li·ªáu c·ªßa m√≥n ph·ªü b√≤.", height=100)
        img = None

    # ===== Mode IMAGE =====
    else:
        user_desc = ""  # khi ·ªü Image mode, kh√¥ng d√πng m√¥ t·∫£
        image_file = st.file_uploader("T·∫£i ·∫£nh m√≥n ƒÉn (PNG/JPG/JPEG)", type=["png","jpg","jpeg"])
        img = None
        if image_file is not None:
            img = Image.open(image_file)
            # Hi·ªÉn th·ªã ·∫£nh thu nh·ªè trong expander (m·∫∑c ƒë·ªãnh ƒë√≥ng) ƒë·ªÉ UI g·ªçn
            with st.expander("üì∑ ·∫¢nh ƒë·∫ßu v√†o (thu nh·ªè)", expanded=False):
                thumb = img.copy()
                thumb.thumbnail((320, 320))
                st.image(thumb, use_container_width=False)

    if run_extract:
        t0 = time.time()
        try:
            if input_mode == "Text":
                if not user_desc.strip():
                    st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ m√≥n ƒÉn.")
                else:
                    raw_response = invoke_text(user_desc, float(temperature), int(max_tokens))
                    mode_label = "MOCK (text)" if MOCK_MODE else "LIVE (text)"
                    render_result(raw_response, t0, mode_label)
            else:
                if img is None:
                    st.warning("Vui l√≤ng t·∫£i ·∫£nh m√≥n ƒÉn.")
                else:
                    # ·ªû Image mode: d√πng prompt ·∫£nh tƒÉng c∆∞·ªùng trong build_prompt_with_image
                    raw_response = invoke_image(
                        desc="",  # ƒë·ªÉ build_prompt_with_image t·ª± ch√®n h∆∞·ªõng d·∫´n m·∫∑c ƒë·ªãnh cho ·∫£nh
                        img=img,
                        temperature=float(temperature),
                        max_tokens=int(max_tokens),
                    )
                    mode_label = "LIVE (image)" if (bedrock_client and LIVE_IMAGE_ALWAYS and MODEL_ID) else "MOCK (image)"
                    render_result(raw_response, t0, mode_label)
        except Exception as e:
            st.error(f"‚ùå L·ªói x·ª≠ l√Ω: {e}")



with tab_bench:
    st.subheader("So s√°nh nhi·ªÅu model (gi·∫£ l·∫≠p khi MOCK)")
    st.caption("Ch·ªçn model ‚Üí ch·∫°y benchmark ‚Üí xem b·∫£ng + bi·ªÉu ƒë·ªì. ·ªû MOCK, s·ªë li·ªáu minh ho·∫° ph·ª•c v·ª• b√°o c√°o.")

    available_models = [
        "anthropic.claude-3-5-sonnet-20240620-v1:0",
        "ai21.jamba-1-5-mini-v1:0",
        "amazon.nova-lite-v1:0",
    ]
    selected_models = st.multiselect("Ch·ªçn model", available_models, default=available_models[:2])

    cc1, cc2, _ = st.columns([1,1,2])
    with cc1:
        bench_temp = st.number_input("Temperature (bench)", 0.0, 1.0, float(TEMPERATURE), 0.1)
    with cc2:
        bench_max = st.number_input("Max tokens (bench)", 64, 4096, int(MAX_TOKENS), 64)

    user_desc_bench = st.text_area("M√¥ t·∫£ d√πng benchmark", value="H√£y cho t√¥i nguy√™n li·ªáu c·ªßa m√≥n ph·ªü b√≤.", height=80)
    run_bench = st.button("Benchmark models")

    if run_bench:
        rows = benchmark_models(user_desc_bench, selected_models, float(bench_temp), int(bench_max))

        tab_overview, tab_charts = st.tabs(["üìã Overview", "üìà Charts"])
        with tab_overview:
            sort_metric = st.selectbox(
                "S·∫Øp x·∫øp theo",
                options=["latency_s", "valid_rate", "cost_est_usd", "tokens_in", "tokens_out", "model"],
                index=0
            )
            ascending = st.checkbox("S·∫Øp x·∫øp tƒÉng d·∫ßn", value=(sort_metric not in ["valid_rate"]))
            rows_sorted = sorted(rows, key=lambda r: r.get(sort_metric, 0), reverse=not ascending)
            st.dataframe(rows_sorted, use_container_width=True)

        with tab_charts:
            try:
                import matplotlib.pyplot as plt

                labels = [r["model"] for r in rows]
                lat = [r["latency_s"] for r in rows]
                val = [r["valid_rate"] for r in rows]
                cost = [r["cost_est_usd"] for r in rows]

                apply_same_order = st.checkbox("√Åp d·ª•ng th·ª© t·ª± s·∫Øp x·∫øp gi·ªëng Overview", value=True)
                if apply_same_order:
                    labels = [r["model"] for r in rows_sorted]
                    lat = [r["latency_s"] for r in rows_sorted]
                    val = [r["valid_rate"] for r in rows_sorted]
                    cost = [r["cost_est_usd"] for r in rows_sorted]

                with st.expander("‚è±Ô∏è Latency (s)", expanded=True):
                    fig1 = plt.figure()
                    plt.bar(labels, lat)
                    plt.title("Latency (s)")
                    plt.xticks(rotation=15, ha='right')
                    st.pyplot(fig1, use_container_width=True)

                with st.expander("‚úîÔ∏è Valid rate (0-1)", expanded=False):
                    fig2 = plt.figure()
                    plt.bar(labels, val)
                    plt.title("Valid rate (0-1)")
                    plt.xticks(rotation=15, ha='right')
                    st.pyplot(fig2, use_container_width=True)

                with st.expander("üí≤ ∆Ø·ªõc t√≠nh chi ph√≠ (USD)", expanded=False):
                    fig3 = plt.figure()
                    plt.bar(labels, cost)
                    plt.title("Estimated Cost (USD)")
                    plt.xticks(rotation=15, ha='right')
                    st.pyplot(fig3, use_container_width=True)
            except Exception as e:
                st.info(f"Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì: {e}")

st.divider()
